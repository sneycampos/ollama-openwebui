services:
    ollama:
      image: ollama/ollama
      container_name: ollama
      ports:
        - "11434:11434"
      volumes:
        - ollama:/root/.ollama
      # Comment out the following lines if you don't have a GPU
      deploy:
        resources:
          reservations:
            devices:
              - capabilities: [gpu]

    open-webui:
      restart: always
      image: ghcr.io/open-webui/open-webui:main
      container_name: open-webui
      environment:
        - OLLAMA_BASE_URL=http://ollama:11434
      ports:
        - "8080:8080"
      volumes:
        - open-webui:/app/backend/data
      depends_on:
        - ollama

volumes:
  ollama:
  open-webui: